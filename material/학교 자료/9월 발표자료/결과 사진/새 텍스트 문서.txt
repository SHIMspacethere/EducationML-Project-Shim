tf.keras.layers.Embedding(input_dim=100, output_dim=64, input_length=X.shape[1])

tf.keras.layers.Embedding은 텐서플로우에서 제공하는 임베딩 레이어입니다. 임베딩은 일반적으로 자연어 처리에서 단어나 문장을 고정된 크기의 벡터로 변환하는 과정을 의미합니다. 이렇게 변환된 벡터는 원-핫 인코딩보다 훨씬 더 효율적으로 단어나 문장의 의미를 포착할 수 있습니다.

Embedding 레이어의 주요 파라미터는 다음과 같습니다:

input_dim: 입력 단어의 최대 인덱스 + 1. 즉, 어휘 사전의 크기입니다. 예를 들어, 단어 인덱스가 0부터 99까지 있다면 input_dim은 100이 됩니다.

output_dim: 임베딩 벡터의 차원입니다. 이는 하이퍼파라미터로, 어떤 값도 가능합니다. 하지만 일반적으로 50, 100, 200, 300 등을 사용합니다.

input_length: 입력 시퀀스의 길이입니다. 여기서는 패딩된 시퀀스의 길이(X.shape[1])가 됩니다.

output_dim을 주로 바꾸게 되는데, 
논문에서는 한국 어휘 특성상 단어 벡터 크기가 300 차원일 경우 좋은 결과를 보였다.



tokenizer = Tokenizer(num_words=100, oov_token="<OOV>") 

Tokenizer에서 num_words는 100개면 빈도가 높은 단어 100개만 선정하고, 나머지는 "OOV"로 처리한다.



tf.keras.layers.Dense(128, activation='relu'),

이 코드 라인은 Keras를 사용해 인공신경망의 한 계층을 정의하는 부분입니다.

tf.keras.layers.Dense(128, activation='relu')
여기서 각 요소의 의미는 다음과 같습니다:

Dense: 완전 연결 레이어를 의미합니다. 각 노드가 이전 레이어의 모든 노드와 연결되어 있다는 것을 의미합니다.
128: 이 레이어에는 128개의 노드(뉴런)이 있다는 것을 의미합니다.
activation='relu': 활성화 함수로 ReLU(Rectified Linear Unit)를 사용한다는 것을 의미합니다. ReLU 함수는 음수를 0으로 바꾸고, 양수는 그대로 둡니다. 이 함수는 일반적으로 비선형성을 도입하고, 모델이 복잡한 패턴을 학습할 수 있게 해줍니다.
이 레이어는 입력을 받아 128차원의 공간으로 변환하는데, 이 변환은 처음에는 무작위로 이루어지다가 학습 과정에서 점점 최적화됩니다. ReLU 활성화 함수가 적용되어 각 노드의 출력을 결정하게 됩니다.