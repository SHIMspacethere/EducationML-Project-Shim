  url = {http://improbable.com/airchives/paperair/volume12/v12i5/chicken-12-5.pdf},
@article{cite:grid,
  title={Grid Search, Random Search, Genetic Algorithm: A Big Comparison for NAS},
  author={Liashchynskyi, Petro and Liashchynskyi, Pavlo},
  journal={ar5iv},
  year={2019},
  url={https://ar5iv.labs.arxiv.org/html/1912.06059}
}

@inproceedings{cite:word2vec,
  title={Large-Scale Text Classification Methodology with Convolutional Neural Network},
  author={Hwiyeol Jo and Jinhwa Kim and Sangwoong Yoon and Kyungmin Kim and Byoung-Tak Zhang},
  booktitle={2015 Winter Conference},
  organization={Department of Computer Science and Engineering, College of Engineering, Seoul National University; Interdisciplinary Program in Cognitive Science, College of Humanities, Seoul National University; Interdisciplinary Program in Neuroscience, College of Natural Sciences, Seoul National University},
  year={2015},
  email={hyjo, jhkim, swyoon, kmkim, btzhang}@bi.snu.ac.kr
}

@Article{cite:lms,
AUTHOR = {Balkaya, Selen and Akkucuk, Ulas},
TITLE = {Adoption and Use of Learning Management Systems in Education: The Role of Playfulness and Self-Management},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {1127},
URL = {https://www.mdpi.com/2071-1050/13/3/1127},
ISSN = {2071-1050},
ABSTRACT = {This article investigates the factors affecting primary and secondary education teachers’ behavioral intention to adopt learning management systems (LMSs). Information technology (IT) innovations have the power to change the way we work, educate, learn, and basically the way we live. The effect of IT innovations on education makes it critical to understand the current usage situation of LMSs and the factors affecting their adoption by teachers. The unified theory of acceptance and use of technology (UTAUT) was extended with factors from education and game-based learning literature. In order to see the effect of individual- and organizational-level characteristics, multi-group structural equation modeling (SEM) analysis was conducted and discrepancies in relationships were reported. Evaluation of users and non-users and teachers of different fields were also compared to each other. The findings of this study not only contribute to theory through the development and testing of a thorough model relating technology features and individual characteristics to behavioral intention to use, but also offer strong implications for practitioners who would like to increase LMS usage and create a more effective learning environment.},
DOI = {10.3390/su13031127}
}

@inproceedings{cite:textClassification1,
  author = {Seong-Yoon Shin and Kwang-Seong Shin and Hyun-Chang Lee},
  title = {Deep Learning Model-based Text Classification},
  booktitle = {Proceedings of the Korea Information and Communications Society Conference},
  year = {2020},
  note = {July},
}

@article{cite:overfitting,
  title={Machine Learning Students Overfit to Overfitting},
  author={Valdenegro-Toro, Matias and Sabatelli, Matthia},
  journal={arXiv preprint arXiv:2209.03032},
  year={2022}
}

@article{cite:CategoricalCrossentropy,
  title={Uses and Abuses of the Cross-Entropy Loss: Case Studies in Modern Deep Learning},
  author={Elliott Gordon-Rodriguez and Gabriel Loaiza-Ganem and Geoff Pleiss and John P. Cunningham},
  journal={arXiv preprint arXiv:2011.05231},
  year={2020},
  url={https://ar5iv.org/abs/2011.05231}
}

@mastersthesis{cite:CentralLimit,
  author       = {Koo, Kyung-Hee},
  title        = {A Study on the Central Limit Theorem},
  school       = {Major in Mathematics Education},
  year         = {2008},
  type         = {Domestic Master's Thesis},
  pages        = {i, 55},
}

@article{cite:softmax,
  author       = {Ihor Vasyltsov and
                  Wooseok Chang},
  title        = {Efficient Softmax Approximation for Deep Neural Networks with Attention
                  Mechanism},
  journal      = {CoRR},
  volume       = {abs/2111.10770},
  year         = {2021},
  url          = {https://arxiv.org/abs/2111.10770},
  eprinttype    = {arXiv},
  eprint       = {2111.10770},
  timestamp    = {Fri, 26 Nov 2021 13:48:43 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2111-10770.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cite:handwriting1,
  author = {Suh Jae Hyeon and Lim Heui Seok},
  title = {A Study on the Mathematical Expression Recognition of Online Handwriting Data Consisting of Coordinate Pairs},
  booktitle = {Proceedings of the Korea Artificial Intelligence Conference},
}

@inproceedings{cite:split,
    title = "We Need to Talk about Standard Splits",
    author = "Gorman, Kyle  and
      Bedrick, Steven",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1267",
    doi = "10.18653/v1/P19-1267",
    pages = "2786--2791",
    abstract = "It is standard practice in speech {\&} language technology to rank systems according to their performance on a test set held out for evaluation. However, few researchers apply statistical tests to determine whether differences in performance are likely to arise by chance, and few examine the stability of system ranking across multiple training-testing splits. We conduct replication and reproduction experiments with nine part-of-speech taggers published between 2000 and 2018, each of which claimed state-of-the-art performance on a widely-used {``}standard split{''}. While we replicate results on the standard split, we fail to reliably reproduce some rankings when we repeat this analysis with randomly generated training-testing splits. We argue that randomly generated splits should be used in system evaluation.",
}

@inproceedings{cite:TextRecognition1,
   title={CMFN: Cross-Modal Fusion Network for Irregular Scene Text Recognition},
   ISBN={9789819980765},
   ISSN={1611-3349},
   url={http://dx.doi.org/10.1007/978-981-99-8076-5_31},
   DOI={10.1007/978-981-99-8076-5_31},
   booktitle={Lecture Notes in Computer Science},
   publisher={Springer Nature Singapore},
   author={Zheng, Jinzhi and Ji, Ruyi and Zhang, Libo and Wu, Yanjun and Zhao, Chen},
   year={2023},
   month=nov, pages={421–433} }

@misc{cite:jiang2024quick,
      title={A Quick Framework for Evaluating Worst Robustness of Complex Networks}, 
      author={Wenjun Jiang and Peiyan Li and Tianlong Fan and Ting Li and Chuan-fu Zhang and Tao Zhang and Zong-fu Luo},
      year={2024},
      eprint={2403.00027},
      archivePrefix={arXiv},
      primaryClass={cs.SI}
}

@misc{cite:cui2024deep,
      title={Deep Network for Image Compressed Sensing Coding Using Local Structural Sampling}, 
      author={Wenxue Cui and Xingtao Wang and Xiaopeng Fan and Shaohui Liu and Xinwei Gao and Debin Zhao},
      year={2024},
      eprint={2402.19111},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

@misc{cite:zhang2024teleclass,
      title={TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision}, 
      author={Yunyi Zhang and Ruozhen Yang and Xueqiang Xu and Jinfeng Xiao and Jiaming Shen and Jiawei Han},
      year={2024},
      eprint={2403.00165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{cite:SamplingMethods,
author = {Makwana, Dhaval and Engineer, Priti and Dabhi, Amisha and Hardik, Chudasama},
year = {2023},
month = {06},
pages = {762-768},
title = {Sampling Methods in Research: A Review},
volume = {7}
}

@article{cite:SplittingData,
author = {Toleva, Borislava},
year = {2021},
month = {05},
pages = {},
title = {The Proportion for Splitting Data into Training and Test Set for the Bootstrap in Classification Problems},
volume = {12},
journal = {Business Systems Research Journal},
doi = {10.2478/bsrj-2021-0015}
}

@article{cite:SimpleRandeomSampling,
author = {Noor, Shagofah and Tajik, Omid and Golzar, Jawad},
year = {2022},
month = {12},
pages = {78-82},
title = {Simple Random Sampling},
volume = {1},
doi = {10.22034/ijels.2022.162982}
}

@article{cite:Lopez-delRio2020EffectOfSequencePadding,
  title={Effect of sequence padding on the performance of deep learning models in archaeal protein functional prediction},
  author={Lopez-del Rio, Angela and Martin, Maria and Perera-Lluna, Alexandre and Saidi, Rabie},
  journal={Scientific Reports},
  volume={10},
  number={1},
  pages={14634},
  year={2020},
  date={2020/09/03},
  issn={2045-2322},
  doi={10.1038/s41598-020-71450-8},
  url={https://doi.org/10.1038/s41598-020-71450-8}
}

@article{cite:Reddy2020EffectOfPadding,
  title={Effects of Padding on LSTMs and CNNs},
  author={Dwarampudi Mahidhar Reddy and N V Subba Reddy},
  year={2020},
  address={Manipal, Karnataka, 576104, India},
  institution={Computer Science and Engineering Department, Manipal Institute of Technology},
  email={mahidhar_d@hotmail.com, nvs.reddy@manipal.edu},
  abstract={This paper focuses on preprocessing techniques for managing variable-length input sequences for LSTM and CNN models, discussing methods like truncation and padding (both pre and post-padding). The study suggests that padding is generally preferable to truncation for converting sequences to equal length, as truncation can lead to data loss. It also found that pre-padding might be more efficient for LSTM models in certain contexts, such as sentiment analysis, because LSTMs remember information from previous outputs. In contrast, the choice between pre-padding and post-padding does not significantly affect CNNs because they focus on pattern recognition rather than sequential memory.},
  url={https://doi.org/10.1038/s41598-020-71450-8}
}

@misc{cite:IntroductionforCNN,
      title={An Introduction to Convolutional Neural Networks}, 
      author={Keiron O'Shea and Ryan Nash},
      year={2015},
      eprint={1511.08458},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@article{cite:OptimizingforCNN,
author = {Ma, Yufei and Cao, Yu and Vrudhula, Sarma and Seo, Jae-sun},
year = {2018},
month = {07},
pages = {1354-1367},
title = {Optimizing the Convolution Operation to Accelerate Deep Neural Networks on FPGA},
volume = {26},
journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
doi = {10.1109/TVLSI.2018.2815603}
}

@misc{cite:Convoultional,
      title={A guide to convolution arithmetic for deep learning}, 
      author={Vincent Dumoulin and Francesco Visin},
      year={2018},
      eprint={1603.07285},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{cite:TokenizerforCNN,
      title={SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing}, 
      author={Taku Kudo and John Richardson},
      year={2018},
      eprint={1808.06226},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{cite:OnlineTokenizer,
      title={iBOT: Image BERT Pre-Training with Online Tokenizer}, 
      author={Jinghao Zhou and Chen Wei and Huiyu Wang and Wei Shen and Cihang Xie and Alan Yuille and Tao Kong},
      year={2022},
      eprint={2111.07832},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{cite:kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{cite:NormalDistribution,
author = {Wesolowski, Brian and Musselwhite Thompson, Dorothy},
year = {2018},
month = {01},
pages = {},
title = {Normal Distribution},
doi = {10.4135/9781506326139.n476}
}


@Article{cite:MathFormulaRecognition,
AUTHOR = {Zhou, Mingle and Cai, Ming and Li, Gang and Li, Min},
TITLE = {An End-to-End Formula Recognition Method Integrated Attention Mechanism},
JOURNAL = {Mathematics},
VOLUME = {11},
YEAR = {2023},
NUMBER = {1},
ARTICLE-NUMBER = {177},
URL = {https://www.mdpi.com/2227-7390/11/1/177},
ISSN = {2227-7390},
ABSTRACT = {Formula recognition is widely used in document intelligent processing, which can significantly shorten the time for mathematical formula input, but the accuracy of traditional methods could be higher. In order to solve the complexity of formula input, an end-to-end encoder-decoder framework with an attention mechanism is proposed that converts formulas in pictures into LaTeX sequences. The Vision Transformer (VIT) is employed as the encoder to convert the original input picture into a set of semantic vectors. Due to the two-dimensional nature of mathematical formula, in order to accurately capture the formula characters’ relative position and spatial characteristics, positional embedding is introduced to ensure the uniqueness of the character position. The decoder adopts the attention-based Transformer, in which the input vector is translated into the target LaTeX character. The model adopts joint codec training and Cross-Entropy as a loss function, which is evaluated on the im2latex-100k dataset and CROHME 2014. The experiment shows that BLEU reaches 92.11, MED is 0.90, and Exact Match(EM) is 0.62 on the im2latex-100k dataset. This paper’s contribution is to introduce machine translation to formula recognition and realize the end-to-end transformation from the trajectory point sequence of formula to latex sequence, providing a new idea of formula recognition based on deep learning.},
DOI = {10.3390/math11010177}
}

@misc{cite:Probability,
      title={Proof mining and probability theory}, 
      author={Morenikeji Neri and Nicholas Pischke},
      year={2024},
      eprint={2403.00659},
      archivePrefix={arXiv},
      primaryClass={math.LO}
}